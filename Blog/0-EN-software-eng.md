# Programming / Quite an Art
*translated by ChatGPT*

Donald Knuth once wrote a very valuable and concise book called *The Art of Computer Programming*, so that analogy is already taken. That's why I‚Äôll start by clarifying what programming is *not*: programming isn‚Äôt about squeezing every last drop of performance out of a computer‚Äôs CPU using machine instructions. Sure, in some very specific cases, that kind of knowledge can be useful ‚Äî but in practice, nearly all code is generated by a compiler, which handles machine code and architectural details far better than a human can. If your last encounter with programming was writing 16-bit assembler for the i8086, you might be shocked at what modern processors are capable of.

In short, programming is about translating loosely defined human ideas and desires into a formal language ‚Äî ideally in a way that keeps complexity and abstraction to a minimum. For example, it's much harder to write a Python program (supposedly the simplest and most inefficient language) based on conversations with a client than to rewrite that same Python program into assembly. The process of formalization forces you to specify all the vague assumptions and edge cases that we tend to overlook when simply describing a problem. Of course, there are better and worse ways to solve problems: one can overcomplicate things, add too many dependencies that become sources of bugs, or just implement something in a very inefficient way. And it "just have to work".

## What Is This Article About?

This article is about the role programming languages have played in shaping the IT world as we know it today. Programming languages, like many (non-)formal standards ‚Äî such as the dominance of QWERTY, social networks, or the VHS vs. Betamax format war ‚Äî didn‚Äôt always win by being the best. Often, success depended on being in the right place at the right time.

Today, we can say that Microsoft was the main beneficiary of those fortunate circumstances. The company started out as a vendor of programming languages and, by focusing on (relative) developer convenience, managed to stay near the front of the pack for the past fifty years ‚Äî something that can't be said for, say, IBM.

Contrary to conspiracy theories, Microsoft didn‚Äôt win just because Bill Gates was well-connected or favored by the establishment. In fact, IBM ‚Äî which ultimately lost its direct battle with Microsoft ‚Äî was, and remains, far more emblematic of the traditional corporate system. IBM, often nicknamed ‚ÄúBig Blue,‚Äù is practically the embodiment of Corporate America.

Another common claim is that Microsoft succeeded because it monopolized the software market. But that's putting the cart before the horse ‚Äî it was precisely because Microsoft became so successful that it was able to dominate the market.

So what was the key to that success? It was a shift in the IT business paradigm: for Microsoft, hardware was secondary ‚Äî merely a vessel for the software. And for good reason: software, which can be copied at virtually zero cost, offers far greater profit potential than hardware, which must be physically manufactured for each sale. Of course, this model brings its own challenges, such as piracy ‚Äî both individual and corporate. The former is simply when users don‚Äôt pay for software. The latter happens when another company copies your product 1:1 and sells it more cheaply, without having to invest in figuring out what the market wants. Still, the economies of scale in software were worth it ‚Äî and were only surpassed later by those of the internet economy.

In the early 1980s, when this story was unfolding, it wasn‚Äôt just a simple case of Microsoft vs. IBM, or Microsoft vs. Apple. Before anyone could sell software, someone had to buy a computer to run it on. It‚Äôs worth noting that at the peak of Microsoft‚Äôs dominance ‚Äî when Windows 95 launched ‚Äî there were even people buying the operating system *before* they owned a computer, thanks to a massive marketing campaign. Of course, such perfect customers were rare.

## A Brief Taxonomy of Computing Machines

Computers have many ancestors: Euclid (for his algorithm to compute the greatest common divisor), the Antikythera mechanism, the automata of Pierre Jaquet-Droz, mechanical differential analyzers (used, for example, to predict tides), and Charles Babbage‚Äôs Analytical Engine (1833‚Äì1871), which was never completed ‚Äî but it was enough to inspire Ada Lovelace (daughter of Lord Byron) to become the world‚Äôs first programmer. Then came Turing‚Äôs theoretical machine, followed by Church‚Äôs lambda calculus. On paper, these models worked ‚Äî which may sound less impressive today ‚Äî but it‚Äôs Turing‚Äôs machine that we consider the foundational definition of a computer. This brings us into the realm of elegant mathematics: primitive recursive functions, finite automata (or those with a stack), and the theory of computability. In short: something is a computer if it supports `IF` and `GOTO`; a programming language is Turing complete if it can express both.

The first practical machines appeared just before World War II: 
- The **Z3** by Konrad Zuse, built in Germany between 1935‚Äì1941,  
- The **Colossus Mark 1**, built at Bletchley Park to break encrypted messages (based on earlier Polish work),  
- The **Atanasoff‚ÄìBerry Computer** (1939).

The Z3 was electromechanical, the Colossus was electronic, and the Atanasoff‚ÄìBerry was fully electronic (using vacuum tubes) ‚Äî but none of them were Turing complete. The first programmable computer in the modern sense was the American **ENIAC** (1943‚Äì1945, in use until 1955), though programming it involved manually rewiring the machine.

After that, things accelerated: in 1954, IBM introduced the IBM 604 ‚Äî the first mass-produced computer using transistors (invented in 1947). In 1958, the first integrated circuits appeared. And in 1964, IBM released the **System/360**, the first computer built entirely with integrated circuits. Computers became smaller, faster, and cheaper. Despite a frequently misquoted remark by IBM‚Äôs Thomas Watson from 1953 ‚Äî ‚Äúthere‚Äôs a world market for maybe five computers‚Äù (which was actually about the number of orders IBM had received at the time) ‚Äî demand continued to grow.

Another major milestone came with the **DEC PDP-8** (1964), the first *minicomputer* ‚Äî think of it as a two-door wardrobe instead of an entire wall of machinery (like a mainframe such as the IBM System/360). DEC and the PDP series are especially memorable because it was on a **PDP-11** at Bell Labs in Massachusetts where Ken Thompson and Dennis Ritchie first created **Unix** (1969), and later the **C programming language** (1972), rewriting Unix in C shortly thereafter.

This was the true beginning of our era. üôÇ

### Home Computers (aka 8-bitters)

The first major breakthrough that everyday users could experience directly was the invention of the microprocessor (1971, Intel 4004), the emergence of the microcomputer, and the arrival of 8-bit home computers based on the MOS 6500 (a clone of the Motorola 6500) and the Zilog Z80. Each machine had its own quirks, but they were all quite similar under the hood:

* **Altair 8800** (1975) ‚Äî a bit primitive, with very limited interactivity. This is where Bill Gates sold his first version of BASIC.
* **Apple I** (1976)
* **Apple II** (1977) ‚Äî the first true home minicomputer
* **Atari 400** (1979) ‚Äî the first microcomputer from a mass-market producer. Atari had grown in the '70s by selling game consoles.
* **Commodore 64** (1982), successor to the Commodore PET (1977) ‚Äî the most popular home computer of all time.
* **ZX Spectrum** (1982) ‚Äî a budget-conscious British machine, built cheaply and with mass affordability in mind.
* **Amstrad CPC 464** (1984), particularly popular in Germany ‚Äî marking, more or less, the end of the 8-bit era.

This was the moment when computers ‚Äúreached the masses.‚Äù A grassroots computing scene emerged: tech magazines flourished, people copied tapes with programs, typed in games from printed listings, and used magical `POKE` commands combined with BASIC‚Äôs `GOTO` logic to create the perfect recipe for one-off hobbyist programming.

From a market perspective, this was a fascinating time ‚Äî a highly fragmented landscape of products, all loosely unified by the use of BASIC (though often incompatible versions), and a world in which no one was quite ready to start paying for software.

### IBM PC (1981): The Empire Strikes Back ‚Äî With 16 Bits

In 1981, ‚ÄúBig Blue‚Äù (IBM) finally made its move into the microcomputer market ‚Äî and it did so decisively. The IBM PC was launched with the 16-bit Intel 8086 processor (introduced in 1978). This was the beginning of the architecture that, in large part, still survives today in the form of the BIOS (at least if UEFI is disabled). Despite the name *Personal Computer*, the IBM PC was not really aimed at hobbyists or home users, but rather at commercial customers ‚Äî more of a workstation than a toy.

The IBM PC shipped with Microsoft‚Äôs version of BASIC pre-installed in ROM and ran MS-DOS, which was itself an improved version of the CP/M system popular on 8-bit machines. That improved version was written by Tim Paterson of Seattle Computer Products, who sold the rights to Bill Gates for $75,000 ‚Äî which would be closer to $750,000 today, adjusted for housing market inflation.

One of the key reasons for the PC's commercial success was that IBM left much of it unpatented. This allowed other, often cheaper, manufacturers ‚Äî both from the US and from East Asia ‚Äî to create compatible clones. That decision became a cornerstone of how the market evolved: once everyone started copying IBM, it no longer made sense to copy anything else. Competing approaches, like Apple‚Äôs tightly integrated ecosystem, were much harder to replicate.

Apple‚Äôs story has been told in more films than the rest of computing history combined, so we won‚Äôt dwell on the brilliance of Wozniak, Jobs‚Äô flair, or who played which role in the corporate battles of the 1980s. What matters is this: Apple chose the 16-bit Motorola 68K as its processor and built the Macintosh around a graphical user interface (GUI). 

The inspiration for that GUI came from Xerox PARC, whose researchers had developed a visionary prototype in 1973 ‚Äî the Xerox Alto ‚Äî featuring a mouse, GUI, laser printer, network connection, and even a camera. Xerox, flush with money from its copier business, gave its Palo Alto team room to dream up the computer of the future. That prototype directly inspired both Steve Jobs and Bill Gates.

The Apple Macintosh was launched in 1984. To his credit, Jobs pushed his team to transform Xerox‚Äôs rough prototype into a polished, elegant machine. Yes, it overheated ‚Äî a reflection of Jobs‚Äô obsession with form over function ‚Äî and no, it wasn‚Äôt built for easy hardware upgrades. But to be fair, even back then, most users didn‚Äôt upgrade their machines. Unfortunately, it turned out to be too expensive ‚Äî and too closed off to third-party developers and clone manufacturers.

IBM clones, on the other hand ‚Äî made by now-forgotten companies like NEC, Gateway, and Compaq ‚Äî were 100% compatible with the original PC. You could swap expansion cards, transfer software, and reuse peripherals. In fact, some innovations from the clones ‚Äî like the ‚Äúturbo‚Äù button or the Hercules graphics card ‚Äî were eventually adopted by IBM itself.

Apple's competitors took a different route: instead of cloning just the machine, they cloned the entire ecosystem. Two key examples are worth noting: the **Atari ST** (1986), designed by Jack Tramiel, and the **Amiga 1000** from Commodore. Both machines had their own graphical operating systems and ran on Motorola‚Äôs 68K processor ‚Äî which, objectively speaking, outperformed Intel‚Äôs chips at the time.

Meanwhile, IBM released updated versions of its PC: the **PC XT** (1983) based on the Intel 8088, and the **PC AT** (1987) with the Intel 80286. Technically, both were still lagging behind competitors.

This is when Microsoft re-entered the picture by releasing a graphical interface layer called **Windows** in 1985. Although early Windows versions were behind Apple's system in both appearance and usability, they had one massive advantage: they ran on the largest platform ‚Äî the PC ecosystem. And just to set the record straight, **Norton Commander** didn‚Äôt appear until 1986.

After the success of the first IBM PC, Microsoft and IBM collaborated on a new operating system called **OS/2**. It was intended to be IBM‚Äôs modern, multitasking, GUI-based answer to the rising competition from Apple, Atari, and Commodore. So what went wrong?

Contrary to myth, it wasn‚Äôt sabotage by Microsoft or a conspiracy to undermine IBM. Rather, IBM overpromised to its customers. One of its key claims was that the new OS would support multitasking on **PC AT** systems (based on the Intel 80286).

### Processors and OS/2

To understand what really happened ‚Äî and why some things couldn't be implemented the way IBM‚Äôs corporate management had planned (based on overly optimistic internal reports) ‚Äî we need to take a short detour.

Let‚Äôs start with a brief history of Intel‚Äôs processors. All 16-bit and newer Intel CPUs were backward-compatible. The 8086 (1978) and 8088 (1979) operated in what‚Äôs called *real mode*, which supported up to 1MB of RAM. When the 80286 was introduced in 1982, it increased addressable memory to 16MB (though accessing memory beyond 640KB involved awkward workarounds like EMS and XMS). More importantly, the 286 introduced *protected mode*, which enabled memory isolation between processes and preemptive multitasking. The problem? The 286 was an awkward transitional chip ‚Äî harder to work with than its fully 32-bit successor, the 80386 (1985), which made development significantly more difficult.

Meanwhile, Motorola had already released its 68K processor in 1979 ‚Äî a hybrid 16/32-bit CPU with built-in support for multitasking ‚Äî just after Intel‚Äôs 8086. But since IBM had committed to Intel, a multitasking OS like OS/2 couldn‚Äôt materialize right away. It was more complex to develop, and it wasn‚Äôt until the third generation of Intel processors that the PC platform was finally ready for such features.

Additionally, OS/2 fell victim to what's known as the *second-system effect* ‚Äî a common engineering pitfall where a team, following a successful prototype, tries to design a follow-up system by including all the features they ever dreamed of‚Ä¶ which often leads to unmanageable complexity. Notable examples of this problem include Netscape 6.0, Windows Vista, and the never-released successor to Apple‚Äôs System 7.

Microsoft, for its part, recognized early on that the GUI was the key to winning the personal computing market. They couldn‚Äôt afford to wait indefinitely for IBM and Intel to deliver a machine that met every technical, marketing, and customer requirement. Interestingly, Microsoft was once invited to write a version of Windows for the **Atari ST**, but the project fell apart due to a clash of personalities between the leadership of the two companies. Atari, in trying to copy Apple‚Äôs closed ecosystem, made similar strategic mistakes. Ironically, their machines were among the best on the market in terms of performance per dollar ‚Äî so if Jack Tramiel (who had left Commodore after a boardroom conflict) had been more open to collaboration, things might have gone very differently.

> For context: the Atari ST was popular for many years among electronic musicians because of its strong MIDI support, while the Amiga became a favored platform for gaming thanks to its advanced graphics chipset.

The first version of OS/2 (text-only) didn‚Äôt appear until 1987. Alongside Minix, it remains one of the few systems ever designed specifically for the 80286. A graphical layer, called **Presentation Manager**, was introduced in 1988 ‚Äî around the same time as Windows 2.1. Early versions of Windows were clunky and lacked true multitasking ‚Äî but they were available. OS/2, meanwhile, missed its window of opportunity, even though IBM continued to invest in it well into the late 1990s.

### The 1990 Turning Point ‚Äî The Game Is On

Let‚Äôs briefly recap what the consumer software landscape looked like around 1989‚Äì1990:

* **Norton Commander** was already in use ‚Äî a tool whose importance is largely forgotten by modern users.
* **Windows 3.0** launched in 1990 ‚Äî the first version of Windows to offer real multitasking, thanks to the 80386 processor.
* In 1991, **AMD** released a much cheaper (and technically unlicensed) clone of the 80386 ‚Äî the AMD 386DX running at 40 MHz, which significantly outperformed Intel‚Äôs offering.
* **Amiga** had its flagship Amiga 1000 and the more affordable Amiga 500, both running the graphical Amiga OS.
* **Atari ST** was available, though hampered by its unstable GEM operating system.
* **Apple** had already released four generations of Macintosh computers, and in 1991 launched **System 7**, which became the foundation of the ‚Äúclassic‚Äù Mac era ‚Äî solidifying Apple‚Äôs dominance in publishing and design, especially with the release of **Photoshop** in 1987.
* Microsoft had released **Word** for DOS in 1985 and Word for Windows in 1989. **Excel** came out in 1987. **Windows 3.1**, an improved and more stable version of Windows 3.0, arrived in 1992.
* Meanwhile, in the business world, the dominant office software was still **Lotus 1-2-3** (1983) and **WordPerfect** (1982).

**In a world full of unreliable IBM PC clones, unstable Windows 3.0, and expensive (Apple) or cheap-but-niche (Amiga) alternatives, Microsoft‚Äôs decision to bet on the technically inferior but largest platform ‚Äî the PC ‚Äî didn‚Äôt look like an obvious win.** Once again, we see a missed opportunity for collaboration between the key players of that era.

So how did Microsoft not only survive but eventually dominate ‚Äî so thoroughly that by 1995, with the launch of Windows 95, it completely reshaped the consumer computer market?

We already know one key: IBM‚Äôs open hardware architecture.  
The second key? **Software piracy** ‚Äî widespread on the PC platform. While it didn‚Äôt bring Microsoft direct profits, it vastly expanded the user base.  
And the third key ‚Äî often overlooked ‚Äî was the availability of **developer tools**.

## A Short History of (Pro)gramming

The first programming language was symbolic assembly ‚Äî replacing raw binary instructions with mnemonics like `MOV` or `JMP`. The second, and still in use today, was **Fortran** (1957), based on an earlier system called **Speedcoding** (1953). Other early language designs, like **Flow-Matic** (1955, which led to COBOL), **Plankalk√ºl** by Zuse (1942‚Äì45), and **Superplan** (1952), were more theoretical frameworks than widely used languages. Around the same time as Fortran, **COBOL** (1959) and **Lisp** (1958) also emerged. Fortran was designed for engineers, COBOL for business, and Lisp for researchers working on artificial intelligence. If you also include **Sketchpad** (1963) by Ivan Sutherland, most of today‚Äôs core programming paradigms were already taking shape back then.

Of these, Lisp has survived in nearly its original form. COBOL became the subject of jokes even in the 1990s, while Fortran ‚Äî arguably the most practical of the three ‚Äî still lives on in countless descendants. Fortran introduced things like the infamous `GOTO`, arrays, and line numbers. Its aim was to help write efficient programs a bit more easily than with raw assembler. By contrast, Lisp and COBOL focused on saving the *programmer‚Äôs* time rather than the CPU‚Äôs.

Europe‚Äôs answer to Fortran was **Algol** (1958), whose clear and readable syntax inspired later languages like **Pascal**, **Simula**, and **C**. Algol was expressive enough to implement high-performance numerical algorithms, while also being significantly easier to read and reason about.

And of course, we must mention **BASIC** (1960), which simplified Fortran‚Äôs syntax and kept its core idea: a program is a list of numbered lines.

Fortran, BASIC, and to a lesser extent Lisp and Algol, share three key characteristics:

1. They had no built-in data abstraction mechanisms. The most complex structure available was an array of numbers. Algol began to change this by introducing *records* (struct-like types).
2. The only code abstraction mechanism was procedures (and `GOTO`), which made true modularity nearly impossible beyond simple binary separation.
3. All memory management was manual. Lisp was the exception ‚Äî it introduced garbage collection (GC) in 1959. But for a long time, it remained the only language with that feature. Interestingly, **BASIC included GC for string variables**, which was one reason for its popularity.

While many programmers claim that ‚Äúyou‚Äôre not a real programmer without C and pointers,‚Äù most of us are accustomed to at least using `structs`, which allow bundling together heterogeneous data types. Without that, you just have a pile of loose variables ‚Äî and it‚Äôs all too easy to introduce subtle bugs. For example, in BASIC, any unused string automatically becomes a numeric variable with a value of `0`. A simple typo in an `IF NOT` condition can silently change the behavior ‚Äî since `0` means false and `-1` means true.

That‚Äôs why **Pascal** (by Niklaus Wirth, 1970) and **C** (1972) were such breakthroughs. If you associate Pascal only with boring high school classes, you're missing the point ‚Äî it was a big step toward accessible, readable programming. A program written in Pascal could actually be understood just by reading it! Especially **Turbo Pascal** (1983), which was created by Anders Hejlsberg ‚Äî the same developer behind Delphi, C#, and TypeScript. Turbo Pascal was also an incredibly fast compiler.

In programming books from the early 1990s, you‚Äôll often find a strong focus on imperative languages and compilation models. Compiled programs ran much faster, but compiling took several minutes ‚Äî which slowed productivity in the pre‚ÄìStack Overflow days, where most debugging was done through trial and error.

The strength of C over Pascal wasn‚Äôt the syntax, but the **low-level memory model** (think `void**`), which combined with pseudo-machine-level operations like `i++`, gave more control over the actual execution. Just as importantly, C had **separate compilation units** (`.h` and `.c` files), which made managing larger projects easier.

Writing C or Pascal code for x86 was already difficult ‚Äî but writing graphical user interfaces (GUIs) was an absolute nightmare. Then and now (see JavaScript), GUI programming has always been painful. So, looking back to around 1990, if operating systems with graphical user interfaces were to flourish, they needed a programming language that made UI development bearable.

On Windows, that language was **Visual Basic** (1991).  
Yes ‚Äî seriously.

Visual Basic, an evolution of QuickBASIC, may have retained many of BASIC‚Äôs quirks, but it brought several critical strengths: it was imperative (enabling fast development cycles), it included built-in garbage collection, and it supported a hybrid object-oriented/imperative style. These features made it the **missing piece** that helped Windows succeed. The effort required to build a flawed but functional GUI application in Visual Basic was **orders of magnitude smaller** than doing the same in C or C++ (with all the horrors of MFC, Win32s, and Hungarian notation).

### Objects Open Windows

Students of computer science may remember **Smalltalk** (1980) ‚Äî the first dynamic, object-oriented programming language and a predecessor to Ruby. Its main use case was building graphical user interfaces (GUIs); in fact, it was the first language to offer an object-oriented GUI framework. 

Object-oriented programming predates Smalltalk, though. It began with **Simula** (1962), which introduced the statically-typed object model that was later adopted by **C++** (1985). Around the same time, **Objective-C** (1984) emerged ‚Äî a language that extended C with object-oriented features inspired by Smalltalk‚Äôs dynamic paradigm. 

If you‚Äôve never compared C++ and Objective-C, it‚Äôs worth doing so. Both are object-oriented extensions of C, but their design philosophies are strikingly different. Objective-C played a pivotal role in the birth of **NeXT**, a company founded by Steve Jobs in 1985. NeXT used Objective-C to build its own graphical, object-oriented operating system: **NeXTSTEP**, which ‚Äî combined with FreeBSD ‚Äî became the foundation of **macOS X**.

Jobs understood that a well-designed programming language was essential to the future of GUI-based systems. Unfortunately, companies like Amiga and Atari didn‚Äôt grasp this, and ultimately faded from the market. Apple survived by bringing Jobs back. Unix systems, while powerful, also lagged ‚Äî although the **X11** windowing system was introduced in 1984, it wasn‚Äôt until **Qt/KDE** (1996, in C++) and **GTK/Gnome** (1999, in C using a manually implemented object system called GObject) that Unix finally gained coherent GUI frameworks.

While Objective-C was arguably more elegant than Visual Basic, it had a critical drawback: its memory management relied on a semi-automatic retain/release system, which only worked properly within the event loop ‚Äî and initially, only under cooperative multitasking. This was typical of early GUI systems. Windows, for instance, didn‚Äôt support true multitasking until version 3.0, and only with **Windows 95** did it gain the ability to forcefully terminate misbehaving processes.

Another interesting milestone was **Turbo Vision** (1990), a text-based GUI library from Borland. In countries like Poland, where text-mode Hercules monitors were still common, Turbo Vision became the de facto standard for building office software. From it came **Delphi** (1995), which inherited Visual Basic‚Äôs dominance in rapid Windows GUI development ‚Äî a position it held until the rise of **C#** in 2000.

Today, when we look at object-oriented programming ‚Äî especially on the backend ‚Äî we often feel something is off. It can feel bloated and overly abstract, disconnected from the practical way we'd like to write our programs. Outside the realm of UI, objects can often seem archaic. In C++, for example, many algorithms are more easily and cleanly expressed using **templates** rather than classes.

Why? For one, classic C++‚Äôs object system is quite limited (Objective-C‚Äôs dynamic model is arguably more flexible). And second, when you use generics like `List<T>`, you benefit from **static type safety**, performance, and ease of use ‚Äî compared to plain `List`, where every element must be dynamically unwrapped. Generics provide **compile-time type checking**, which catches errors earlier and leads to more robust software in the long run.

It‚Äôs worth noting that while C++ introduced object-oriented features in 1985, templates ‚Äî which power generics ‚Äî only arrived in 1989 and weren‚Äôt properly standardized for years afterward.

## The Great Convergence (Millennium Edition)

Around the turn of the millennium, we saw the beginning of the **Great Architecture Convergence** ‚Äî the gradual disappearance of all non-mainstream computing platforms:

* **SGI** with its MIPS processors ‚Äì discontinued by 2002 (I once salvaged an SGI Indy from a dumpster; it had a beautiful design)
* **DEC Alpha processors** ‚Äì mostly used by HP and Compaq; phased out between 2004‚Äì2007
* **HP-PA RISC / HP-UX** ‚Äì discontinued by 2008
* **Sun UltraSPARC** ‚Äì discontinued in 1997
* **PowerPC** (IBM, Apple, Motorola) ‚Äì discontinued by Apple in 2006; survived a bit longer in the Xbox 360 and PS3

All of these were classic **RISC architectures**, the philosophical opposite of Intel‚Äôs x86 **CISC**. By the mid-2000s, only **IA-64** (symbolically), **AMD64**, and **ARM** remained standing. The **Motorola 68K** had already fallen earlier when Apple switched to PowerPC.

Meanwhile, in 2001, both **Microsoft Windows XP** and **Apple macOS X** launched ‚Äî marking a shift to fully modern operating systems. Ironically, both were based on similar **microkernel-inspired** designs originally developed for VMS by Dave Cutler.

Two decades later, Apple had its affair with Intel ‚Äî and then left it. Windows now ships with a built-in Ubuntu subsystem. Looking back, the fragmented world of the late 1990s ‚Äî with a half-dozen competing GUI toolkits for Unix, the slow demise of Amiga, and C/C++ still synonymous with ‚Äúprogramming‚Äù ‚Äî feels like the Wild West, a time when *everything* still seemed possible.

## Today

Strangely enough, even though we now have amazing programming models like **Haskell**, **OCaml**, **TypeScript**, and **Erlang**, **GUI programming remains a painful experience**. Concurrency is still a balancing act: between Erlang‚Äôs fast, elegant, and dynamic approach, and C‚Äôs universal ‚Äî but notoriously error-prone ‚Äî model.

A fully parallel version of OCaml didn‚Äôt emerge until 2025. We‚Äôve also seen the rise of a few niche or ‚Äúesoteric‚Äù languages like **Zig** and **Pony**, and the bold (and mostly successful) attempt to replace C with **Rust**.

Assembler has also changed: there‚Äôs now a wide gap between instructions like `REP MOVSW` and what the CPU *actually* executes. Outside of Arduino projects and aging microcontrollers, hardly anyone touches assembly anymore ‚Äî unless they‚Äôre learning **CUDA**.

Most compilers now target **LLVM**, and native binaries are increasingly rare. Everything goes through **bytecode** and **JIT compilation**. Meanwhile, **Python**, with its sugar-sweet syntax, continues to dominate more corners of the industry. And only when compiling a `pip` package from source do we get a glimpse of what‚Äôs really going on: Fortran code turned into BLAS, connected via C bindings, accelerated with CUDA, and finally orchestrated by **Keras**, itself running atop **TensorFlow**.

## Afterword

This article has two origins. 

The first is a personal one ‚Äî rediscovering the joy of tinkering with **DOSBox**, and remembering how even the smallest abstractions ‚Äî like building a `WHILE` loop using `GOTO` and reading from `DATA` lines ‚Äî used to feel magical. Back then, languages like BASIC were a chaotic blend of everything: commands for logic, sound, graphics, and file operations all lived at the same level of abstraction as `GOTO`. And everything was an uphill battle.

The second is a broader question: **How do we get kids interested in programming?** There‚Äôs no one clear answer, but at least **three equally valid paths** stand out:

1. **Interactive programming**, like the classic turtle in **LOGO**. Nothing is as rewarding as making something that *does* something ‚Äî like a simple game you can challenge your friends with.
2. **Classical algorithmics**, the kind you encounter in programming olympiads. In fact, this might be the most practical path today ‚Äî since companies like Google shaped the tech recruitment model around algorithmic thinking. There‚Äôs a huge ecosystem of platforms dedicated to honing this skill: from the early Valladolid platform, to TopCoder, LeetCode, Poland‚Äôs Sphere, and Codility.
3. And finally, **pure computer science** ‚Äî the elegance of functional programming, monads, abstract algebra, and solving problems in a single line of code. When such a line compiles, it becomes an *automatic proof* of correctness. This knowledge may be totally impractical ‚Äî but it‚Äôs pure art for art‚Äôs sake.


# The End of a Chapter: AI and Programming

The revolution of large language models (LLMs) in 2022 showed that AI can actually be *very* good at programming. The **agent/interactive mode** ‚Äî with automatic feedback loops (like in Cursor or GitHub Copilot) ‚Äî performs far better than the **generative mode** (like ChatGPT), and it's clear that AI still makes things up from time to time. Only through interaction with compilers and test suites can most of those errors be eliminated.

Put simply: AI can now solve LeetCode-style problems better than almost anyone (except maybe total outliers). For individual developers working on relatively simple projects, this represents a massive breakthrough and an incredible productivity boost. As **Andrej Karpathy** called it, *‚Äúvibe coding‚Äù* really works ‚Äî at least when it comes to prototyping, it saves countless hours of manual effort.

Let me make a small philosophical detour: from the very beginning, **programming has always been about reducing developer work hours**. Whether you're using STL, GUI builders, or numerical libraries ‚Äî you're saving more time than a single human lifetime could ever match. In that sense, AI is just the next powerful step in this long tradition ‚Äî one more tool that reduces the amount of work required to produce working software. If you think otherwise, try building a modern app ‚Äî with networking and a graphical interface ‚Äî in GW-BASIC or Turbo/QuickBasic. üòÑ

### What is Copilot good at?

It excels at generating code that solves **simple, well-documented problems** ‚Äî often the kind you‚Äôd already find answered on StackOverflow.

### Where does it fall short?

It struggles with **managing complexity**. Auto-generated code tends to be bloated, overly verbose, and sometimes more complicated than the problem calls for. Adding features or adjusting course mid-project often increases this complexity ‚Äî and as mentioned earlier in the article, translating vague user/client expectations into formal specifications remains a significant challenge. You need a clear goal, specific guidance for the AI, and **strong discipline to keep things from spiraling into over-engineered chaos** ‚Äî at least for now.

### What helps AI code better?

The key is **shorter feedback loops**. The faster an AI can detect that it‚Äôs doing something useless, the fewer mistakes it makes ‚Äî and the less it drifts into the weeds. Also, **AI doesn‚Äôt care what language it‚Äôs working in**. It learns any syntax quickly and without bias. That means now is the time for *harder* languages like **OCaml, Haskell, or Rust** to shine. From the AI's point of view, writing clean Python is no easier or harder than writing verbose, assertion-heavy Rust code.

But here's the difference: Python code may let bugs slip through until runtime, while poorly written Rust or Haskell code simply won‚Äôt compile. 

### Conclusions

**We‚Äôre entering an era of strongly statically typed languages**, with as much correctness checking at compile time as possible. A golden age for **functional programming** and other previously ‚Äúesoteric‚Äù languages ‚Äî where the main barrier had been the steep learning curve. But **Copilot Agents** aren‚Äôt intimidated by such complexity. Who knows ‚Äî maybe they‚Äôll finally migrate our legacy codebases from JavaScript to **TypeScript**, or from C++ to **Rust**.


